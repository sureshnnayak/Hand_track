{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2PlazN5Q94l"
      },
      "source": [
        "Usage example of MediaPipe Hands Solution API in Python (see also http://solutions.mediapipe.dev/hands)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuQfLvpuJkb0",
        "outputId": "7e218960-d653-4873-eea2-0b1b28e4dee4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.8.9.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 32.7 MB 33.9 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/lib/python3/dist-packages (from mediapipe) (3.1.2)\n",
            "Collecting protobuf>=3.11.4\n",
            "  Downloading protobuf-3.20.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 133.2 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: attrs>=19.1.0 in /usr/lib/python3/dist-packages (from mediapipe) (19.3.0)\n",
            "Requirement already satisfied: numpy in /usr/lib/python3/dist-packages (from mediapipe) (1.17.4)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages/absl_py-0.13.0-py3.8.egg (from mediapipe) (0.13.0)\n",
            "Collecting opencv-contrib-python\n",
            "  Downloading opencv_contrib_python-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (66.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 66.7 MB 23.0 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/lib/python3/dist-packages (from absl-py->mediapipe) (1.14.0)\n",
            "Installing collected packages: protobuf, opencv-contrib-python, mediapipe\n",
            "Successfully installed mediapipe-0.8.9.1 opencv-contrib-python-4.5.5.64 protobuf-3.20.1\n"
          ]
        }
      ],
      "source": [
        "!pip install mediapipe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWGG66XeRXEi"
      },
      "source": [
        "Upload any image that contains hand(s) to the Colab. We took two examples from the web: https://unsplash.com/photos/QyCH5jwrD_A and https://unsplash.com/photos/mt2fyrdXxzk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38,
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "ok": true,
              "status": 200,
              "status_text": ""
            }
          }
        },
        "id": "7SwzZ1nJLWt0",
        "outputId": "690ee60d-81d2-46e9-e6df-b28204ed5d02"
      },
      "outputs": [],
      "source": [
        "\"\"\"#from google.colab import files\n",
        "import files\n",
        "uploaded = files.upload()\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "import time\n",
        "cam = cv2.VideoCapture(0)\n",
        "\n",
        "return_value, image = cam.read()\n",
        "cam.release()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nW2TjFyhLvVH"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "import cv2\n",
        "import time\n",
        "\n",
        "cam = cv2.VideoCapture(0)\n",
        "\n",
        "def capture_image(i):\n",
        "    return_value, image = cam.read()\n",
        "    cv2.imwrite('opencv'+str(i)+'.jpg', image)\n",
        "cam.release()\n",
        "\n",
        "\n",
        "#import cv2_imshow\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "DESIRED_HEIGHT = 480\n",
        "DESIRED_WIDTH = 480\n",
        "def resize_and_show(image): \n",
        "    cv2.imshow('Input', image)\n",
        "    h, w = image.shape[:2]\n",
        "    if h < w:\n",
        "        img = cv2.resize(image, (DESIRED_WIDTH, math.floor(h/(w/DESIRED_WIDTH))))\n",
        "    else:\n",
        "        img = cv2.resize(image, (math.floor(w/(h/DESIRED_HEIGHT)), DESIRED_HEIGHT))\n",
        "    cv2.imshow('Input1', img)\n",
        "\n",
        "\n",
        "image = cv2.imread('opencv1.jpg')\n",
        "cv2.imshow('Input0', image)\n",
        "\n",
        "\n",
        "resize_and_show(image)\n",
        "cv2.waitKey(5000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Fso4oNqXIkp"
      },
      "source": [
        "All MediaPipe Solutions Python API examples are under mp.solutions.\n",
        "\n",
        "For the MediaPipe Hands solution, we can access this module as `mp_hands = mp.solutions.hands`.\n",
        "\n",
        "You may change the parameters, such as `static_image_mode`, `max_num_hands`, and `min_detection_confidence`, during the initialization. Run `help(mp_hands.Hands)` to get more informations about the parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BboTB-FAMfPo",
        "outputId": "ce264416-a333-4c21-b808-2d54e935bd1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Help on class Hands in module mediapipe.python.solutions.hands:\n",
            "\n",
            "class Hands(mediapipe.python.solution_base.SolutionBase)\n",
            " |  Hands(static_image_mode=False, max_num_hands=2, model_complexity=1, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
            " |  \n",
            " |  MediaPipe Hands.\n",
            " |  \n",
            " |  MediaPipe Hands processes an RGB image and returns the hand landmarks and\n",
            " |  handedness (left v.s. right hand) of each detected hand.\n",
            " |  \n",
            " |  Note that it determines handedness assuming the input image is mirrored,\n",
            " |  i.e., taken with a front-facing/selfie camera (\n",
            " |  https://en.wikipedia.org/wiki/Front-facing_camera) with images flipped\n",
            " |  horizontally. If that is not the case, use, for instance, cv2.flip(image, 1)\n",
            " |  to flip the image first for a correct handedness output.\n",
            " |  \n",
            " |  Please refer to https://solutions.mediapipe.dev/hands#python-solution-api for\n",
            " |  usage examples.\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      Hands\n",
            " |      mediapipe.python.solution_base.SolutionBase\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, static_image_mode=False, max_num_hands=2, model_complexity=1, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
            " |      Initializes a MediaPipe Hand object.\n",
            " |      \n",
            " |      Args:\n",
            " |        static_image_mode: Whether to treat the input images as a batch of static\n",
            " |          and possibly unrelated images, or a video stream. See details in\n",
            " |          https://solutions.mediapipe.dev/hands#static_image_mode.\n",
            " |        max_num_hands: Maximum number of hands to detect. See details in\n",
            " |          https://solutions.mediapipe.dev/hands#max_num_hands.\n",
            " |        model_complexity: Complexity of the hand landmark model: 0 or 1.\n",
            " |          Landmark accuracy as well as inference latency generally go up with the\n",
            " |          model complexity. See details in\n",
            " |          https://solutions.mediapipe.dev/hands#model_complexity.\n",
            " |        min_detection_confidence: Minimum confidence value ([0.0, 1.0]) for hand\n",
            " |          detection to be considered successful. See details in\n",
            " |          https://solutions.mediapipe.dev/hands#min_detection_confidence.\n",
            " |        min_tracking_confidence: Minimum confidence value ([0.0, 1.0]) for the\n",
            " |          hand landmarks to be considered tracked successfully. See details in\n",
            " |          https://solutions.mediapipe.dev/hands#min_tracking_confidence.\n",
            " |  \n",
            " |  process(self, image: numpy.ndarray) -> <class 'NamedTuple'>\n",
            " |      Processes an RGB image and returns the hand landmarks and handedness of each detected hand.\n",
            " |      \n",
            " |      Args:\n",
            " |        image: An RGB image represented as a numpy ndarray.\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If the underlying graph throws any error.\n",
            " |        ValueError: If the input image is not three channel RGB.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A NamedTuple object with the following fields:\n",
            " |          1) a \"multi_hand_landmarks\" field that contains the hand landmarks on\n",
            " |             each detected hand.\n",
            " |          2) a \"multi_hand_world_landmarks\" field that contains the hand landmarks\n",
            " |             on each detected hand in real-world 3D coordinates that are in meters\n",
            " |             with the origin at the hand's approximate geometric center.\n",
            " |          3) a \"multi_handedness\" field that contains the handedness (left v.s.\n",
            " |             right hand) of the detected hand.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from mediapipe.python.solution_base.SolutionBase:\n",
            " |  \n",
            " |  __enter__(self)\n",
            " |      A \"with\" statement support.\n",
            " |  \n",
            " |  __exit__(self, exc_type, exc_val, exc_tb)\n",
            " |      Closes all the input sources and the graph.\n",
            " |  \n",
            " |  close(self) -> None\n",
            " |      Closes all the input sources and the graph.\n",
            " |  \n",
            " |  reset(self) -> None\n",
            " |      Resets the graph for another run.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from mediapipe.python.solution_base.SolutionBase:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import mediapipe as mp\n",
        "mp_hands = mp.solutions.hands\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_drawing_styles = mp.solutions.drawing_styles\n",
        "help(mp_hands.Hands)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BAivyQ_xOtFp"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'images' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/home/smartcane/Desktop/Hand_track/mediapipe_hands.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/smartcane/Desktop/Hand_track/mediapipe_hands.ipynb#ch0000007?line=0'>1</a>\u001b[0m \u001b[39m# Run MediaPipe Hands.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/smartcane/Desktop/Hand_track/mediapipe_hands.ipynb#ch0000007?line=1'>2</a>\u001b[0m \u001b[39mwith\u001b[39;00m mp_hands\u001b[39m.\u001b[39mHands(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/smartcane/Desktop/Hand_track/mediapipe_hands.ipynb#ch0000007?line=2'>3</a>\u001b[0m     static_image_mode\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/smartcane/Desktop/Hand_track/mediapipe_hands.ipynb#ch0000007?line=3'>4</a>\u001b[0m     max_num_hands\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/smartcane/Desktop/Hand_track/mediapipe_hands.ipynb#ch0000007?line=4'>5</a>\u001b[0m     min_detection_confidence\u001b[39m=\u001b[39m\u001b[39m0.7\u001b[39m) \u001b[39mas\u001b[39;00m hands:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/smartcane/Desktop/Hand_track/mediapipe_hands.ipynb#ch0000007?line=5'>6</a>\u001b[0m   \u001b[39mfor\u001b[39;00m name, image \u001b[39min\u001b[39;00m images\u001b[39m.\u001b[39mitems():\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/smartcane/Desktop/Hand_track/mediapipe_hands.ipynb#ch0000007?line=6'>7</a>\u001b[0m     \u001b[39m# Convert the BGR image to RGB, flip the image around y-axis for correct \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/smartcane/Desktop/Hand_track/mediapipe_hands.ipynb#ch0000007?line=7'>8</a>\u001b[0m     \u001b[39m# handedness output and process it with MediaPipe Hands.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/smartcane/Desktop/Hand_track/mediapipe_hands.ipynb#ch0000007?line=8'>9</a>\u001b[0m     results \u001b[39m=\u001b[39m hands\u001b[39m.\u001b[39mprocess(cv2\u001b[39m.\u001b[39mflip(cv2\u001b[39m.\u001b[39mcvtColor(image, cv2\u001b[39m.\u001b[39mCOLOR_BGR2RGB), \u001b[39m1\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/smartcane/Desktop/Hand_track/mediapipe_hands.ipynb#ch0000007?line=10'>11</a>\u001b[0m     \u001b[39m# Print handedness (left v.s. right hand).\u001b[39;00m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'images' is not defined"
          ]
        }
      ],
      "source": [
        "# Run MediaPipe Hands.\n",
        "with mp_hands.Hands(\n",
        "    static_image_mode=True,\n",
        "    max_num_hands=2,\n",
        "    min_detection_confidence=0.7) as hands:\n",
        "  for name, image in images.items():\n",
        "    # Convert the BGR image to RGB, flip the image around y-axis for correct \n",
        "    # handedness output and process it with MediaPipe Hands.\n",
        "    results = hands.process(cv2.flip(cv2.cvtColor(image, cv2.COLOR_BGR2RGB), 1))\n",
        "\n",
        "    # Print handedness (left v.s. right hand).\n",
        "    print(f'Handedness of {name}:')\n",
        "    print(results.multi_handedness)\n",
        "\n",
        "    if not results.multi_hand_landmarks:\n",
        "      continue\n",
        "    # Draw hand landmarks of each hand.\n",
        "    print(f'Hand landmarks of {name}:')\n",
        "    image_hight, image_width, _ = image.shape\n",
        "    annotated_image = cv2.flip(image.copy(), 1)\n",
        "    for hand_landmarks in results.multi_hand_landmarks:\n",
        "      # Print index finger tip coordinates.\n",
        "      print(\n",
        "          f'Index finger tip coordinate: (',\n",
        "          f'{hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].x * image_width}, '\n",
        "          f'{hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].y * image_hight})'\n",
        "      )\n",
        "      mp_drawing.draw_landmarks(\n",
        "          annotated_image,\n",
        "          hand_landmarks,\n",
        "          mp_hands.HAND_CONNECTIONS,\n",
        "          mp_drawing_styles.get_default_hand_landmarks_style(),\n",
        "          mp_drawing_styles.get_default_hand_connections_style())\n",
        "    resize_and_show(cv2.flip(annotated_image, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LAchzK23Uabf"
      },
      "outputs": [],
      "source": [
        "# Run MediaPipe Hands and plot 3d hands world landmarks.\n",
        "with mp_hands.Hands(\n",
        "    static_image_mode=True,\n",
        "    max_num_hands=2,\n",
        "    min_detection_confidence=0.7) as hands:\n",
        "  for name, image in images.items():\n",
        "    # Convert the BGR image to RGB and process it with MediaPipe Hands.\n",
        "    results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "    # Draw hand world landmarks.\n",
        "    print(f'Hand world landmarks of {name}:')\n",
        "    if not results.multi_hand_world_landmarks:\n",
        "      continue\n",
        "    for hand_world_landmarks in results.multi_hand_world_landmarks:\n",
        "      mp_drawing.plot_landmarks(\n",
        "        hand_world_landmarks, mp_hands.HAND_CONNECTIONS, azimuth=5)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "mediapipe_hands.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
